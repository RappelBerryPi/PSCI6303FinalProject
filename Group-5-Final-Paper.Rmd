---
title: 'Final Project: Group 5. Effect of Russian Trolls on the 2016 U.S. Presidential election.'
author: 
    - "Ryan Appel, Angel Arribas Lopez, Drew Breyer, Humza Kahn,"
    - "Devanshi Patel, Hanna Shin, Poonam Siyag"
output:
  pdf_document: default
  html_document:
    df_print: paged
bibliography: "E:\\\\OneDrive - The University of Texas at Dallas\\\\My Library.bib"
---
```{r, include=FALSE}
if (!require("pacman")) install.packages("pacman")
p_load(data.table, dplyr) # two modern packages to modify data objects in R
p_load(stargazer) # a package to summaries regression results
p_load(ggplot2, ggpubr) # two packages to visualize data
p_load(pander)
`%+%` <- function(x,y){paste0(x, y)} # collapse two strings into one
theme_set(theme_pubr(border = TRUE)) # make figures beautiful and ascetic by default
```

```{r, include=FALSE}
# The below function provides a percentage after being passed a part and whole data.table
percentage <- function(part, whole, rounding.digits = 3) {
    part.count <- nrow(part)
    whole.count <- nrow(whole)
    percentage <- part.count / whole.count
    percentage <- percentage * 100
    percentage <- round(percentage, rounding.digits)
    return (percentage)
}

print.percentage <- function(part, whole, rounding.digits = 3) {
    p <- get.percentage(part, whole, rounding.digits)
    print(p)
}

get.percentage <- function(part, whole, rounding.digits = 3) {
    p <- percentage(part, whole, rounding.digits)
    return (p %+% "%")
}

panderOptions('knitr.auto.asis', TRUE)
panderOptions('table.split.table', "Inf")
```

# Impact of a cyberthreat on important outcomes
## Existing Data

Describe the scale of the problem using existing data / opinions from expert reports or academic papers 

> This problem was extremely concerning for the United States and governments around the world, and as a result, there
> are many academic sources and expert reports which looked into the impact that trolls had on the 2016 election. One
> example is out of Tennessee-Knoxville which analyzed 770,005 tweets in English from known Russian troll accounts,
> alongside data from FiveThirtyEight’s archive of multiple polling outlets.

> Some of their findings were alarming, such as that for every 25,000 retweets “Russian accounts correlated to a 1%
> increase in Trump’s poll numbers one week later” as well as seeing that “25,000 retweets would average around 10
> retweets per tweet.” [@ruckInternetResearchAgency2019] Looking at Trump’s opponent and similar social media activity, 
> this kind of retweet volume did not have an impact on her numbers.

> The study additionally looked at how much of this social media activity could be attributed to bots and how much was
> genuine/organic social media reach. The study found that “91% of first retweeters of known Russian bots were non
> Russian bots, which suggests that propaganda spread into the networks of real U.S. citizens.”

Who is the target of the cyberthreat? 

> There were several “targets” of this disinformation campaign. Broadly speaking, the campaign was to sew discord and
> uncertainty into U.S. politics and the election. More specifically, the campaign clearly targeted Donald Trump and
> Hillary Clinton, to improve the likelihood of Donald Trump being elected as President of the United States. As we
> now know, this was successful we can see by the results of the election.
  
Do we know how many targets are affected in the population? What is the proportion of the target group in the overall
population of interest? 

> This is an extremely difficult task to know how many targets were affected. This is due to how social media works,
> as there are many links and ads and links/ads within those, which can then be shared with additional parties.
> Speaking to tweets coming from known Russian troll accounts, which would be far less than the true targeted
> population, one reputable study found there were 770,005 tweets from these accounts, which were then of course
> spread throughout the social media networks.

> Additionally, the more difficult task here is to know who was truly impacted by the social media campaign. That is,
> who was exposed to Russian bot activity AND changed their vote or interest in the election as a result of this material.

## Ideal Conditions

Describe the ideal data you would need to obtain a precise estimate of scale of the problem (proportion of the targeted
units in the overall population of interest)

> The ideal conditions would be to know exactly who was exposed to the Russian bot activity AND changed their vote
> because of having seen the propaganda. As a proportion of the larger population, this would be looking at everyone
> that voted in the U.S. election vs. who was exposed to troll activity and changed their vote, or decided to vote, as
> a result of having seen this material.

What are the major obstacles for collecting these data? 

> Because this is all happening in cyberspace, there are logs and tweets that can tangibly be measured and dissected
> by a statistical approach. That said, there are enormous obstacles as some of this information is neither public or
> available at all. Collecting this data would mean looking into each account and associating the account with a known
> voter identity. Then that person would need to be polled or asked about their affinity for the election i.e. who
> they voted for and why.

> Making things more difficult is that it is hard to tell authentic reach from imitation reach. There are many people
> who have many twitter accounts, and even bots accounts are known to be bought and sold on various marketplaces.
> Discerning what is organic vs what is machine is a burden on this area of research.

Do we know how many targets are affected in the population? What is the proportion of the target group in the overall
population of interest? 

> By one measure, 770,000 messages sent from known Russian troll accounts but reach is difficult to tell because of
> retweets and 43 million other election related tweets sent over same timeline could be difficult to attribute to
> Russia.

## Existing Studies

Describe the major results from existing studies (academic papers or reports) regarding the impact of your cyber threat
on your outcomes of interest

+ Cross-Platform State Propaganda: Russian Trolls on Twitter and Youtube during the 2016 U.S. Presidential Election
  + Troll accounts were primarily trying to help increase support for Donald Trump and conservative candidates.
  + Some accounts were "agnostic" trying to inflame partisan divisions by supporting either side.
+ Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign
  + Conservatives retweeted Russian troll more often producing 36x more tweets
  + Only about 4.9% of conservative users retweeting content were "bots"
+ Describe the data used in these studies
  + 1 - 770,005 tweets from known Russian troll accounts
  + 2 - 43 million elections-related posts shared on Twitter by 5.7 million users (Sept 16-Nov 9 2016)
+ How do authors justify that the relationship between a cyber threat and their outcomes of interest are causal? In
  other words, describe their research design (DiD, RD, IV, matching, naïve regression, etc)
  + Observational: Sorted tweets by their "mean" ideologies using latent semantic analysis
  + DiD through comparing Members of Congress "ideology" to discover categories of the tweets
+ Describe in detail the credibility of these results: potential concerns with data (e.g., selection bias) and the
  research design (e.g., lack/improper control group, violation of parallel trends in DiD, self-sorting in RD, exclusion
  restriction in IV, etc)
  + The findings are credible given they are looking at a subset of the larger population of tweets that they've used
    strong methods to detect "troll-ness" but there is possibility of violation of parallel trends is certainly a
    difficult bar to meet. This requires the absense of treatment in the diffrences over-time which is something the
    authors did not address. This could potentially lead to biased estimation of the causal effect.

## Ideal Experiment
+ The observation can be voters who are in favor of either of Democratic or Republican party and do Twitter
+ Treatment group will be exposed to foreign trolls through Twitter (retweet the foreign trolls)
+ We can code treatment group as 1 if they retweet the foreign trolls
+ Control group can be coded as 0 which means that they are not exposed to foreign trolls (do not retweet the foreign trolls)
+ Our outcome of interest will be the change in voters’ behavior after they are exposed to foreign trolls 
+ After the exposure to foreign trolls, voters’ preferences change
+ It is feasible. We can comprehend voters’ preference based on how many the users have tweeted on the democratic party
  or the republican party. However, we need to know what twitter users are foreign trolls. 
+ We can do Difference in Difference design based on before and after exposure to foreign trolls

## Existing Data Sources
+ We can use dataset which Twitter provides. It has retweet information. We can find that who retweet the foreign trolls
  if we know which users are foreign trolls.
+ Also, we can use Twitter dataset to know potential outcomes of interest. We can find the change in their preference
  based on post and retweet information such as what they post and what they retweet.

## Structure of the Dataset
+ Our structure of a dataset to estimate the impact of the foreign trolls
  + Vote preference 2012 election
  + Vote preference 2016 election
  + Exposure to foreign trolls
+ Treatment variable
  + exposure to foreign trolls
+ Outcomes of interest
  + Vote preference in 2016 election

## Research Design
+ Difference in Difference is part of “Design based inference” quasi experimental models. 
+ Widely used technique amongst economists, social scientists, and researchers.
+ Top model to use for research better than observational studies
+ Difference in difference method does not require randomization. 

|                                   | **2012**            | **2016**          | **Difference**    |
|:----------------------------------|:--------------------|:------------------|------------------:|
| **Exposed to tweets (treatment)** | A (not yet treated) | B (treated)       | B - A             |
| **Not Exposed (control)**         | C (never treated)   | D (never treated) | D - C             |
| **Difference**                    | A - C               | B - D             | (B - A) - (D - C) |

## Code Analysis
```{r}
path <- 'https://raw.githubusercontent.com/RappelBerryPi/PSCI6303FinalProject/main/0-data/data.csv'
d <- fread(path)
```

```{r}
d.display <- d[,vote_changed := as.numeric(vote2012 != vote2016)]
d.display.head <- head(d.display)
colnames(d.display.head) <- gsub("_", " ", colnames(d.display.head))
colnames(d.display.head) <- gsub("troll tweets", "", colnames(d.display.head))
colnames(d.display.head)[6] = "vote 2012"
colnames(d.display.head)[7] = "vote 2016"
pander(d.display.head, style = "rmarkdown")
```

```{r}
d.display[, Total_Tweets := troll_tweets_pro_democratic + troll_tweets_anti_democratic + 
            troll_tweets_pro_democratic + troll_tweets_anti_republican]
mean.total.tweets <- mean(d$Total_Tweets)
d.display[, exposure_to_troll_tweets := Total_Tweets >= mean.total.tweets]
names <- c("Total_Tweets", "vote2012", "vote2016", "vote_changed", "exposure_to_troll_tweets")
d.condensed <- d.display[, ..names]
colnames(d.condensed) <- gsub("_", " ", colnames(d.condensed))
colnames(d.condensed)[2] = "vote 2012"
colnames(d.condensed)[3] = "vote 2016"
pander(head(d.condensed), style = "rmarkdown")
```

## Regression Table Analysis

```{r}
d.aggregate <- d
d.aggregate <- d.aggregate[,troll_tweets_for_republican := troll_tweets_pro_republican + 
                             troll_tweets_anti_democratic]
d.aggregate <- d.aggregate[,troll_tweets_for_democratic := troll_tweets_pro_democratic +
                             troll_tweets_anti_republican]
d.aggregate <- d.aggregate[,troll_tweets_total := troll_tweets_pro_democratic + 
                            troll_tweets_anti_republican + troll_tweets_pro_republican + 
                             troll_tweets_anti_democratic]
d.aggregate <- d.aggregate[, voted_same := vote2012 == vote2016]
mean.total.troll.tweets <- mean(d.aggregate$troll_tweets_total)
mean.for_republican.troll.tweets <- mean(d.aggregate$troll_tweets_for_republican)
mean.for_democratic.troll.tweets <- mean(d.aggregate$troll_tweets_for_democratic)
d.affected.total <- d.aggregate[, affected := troll_tweets_total >= 
                                  mean.total.troll.tweets]
d.affected.republican <- d.aggregate[, affected := troll_tweets_pro_democratic >= 
                                       mean.for_republican.troll.tweets]
d.affected.democratic <- d.aggregate[, affected := troll_tweets_pro_democratic >= 
                                       mean.for_democratic.troll.tweets]
d.affected.republican[, change_to_republican := vote2016 == "R" & voted_same == FALSE]
d.affected.democratic[, change_to_democratic := vote2016 == "D" & voted_same == FALSE]
d.affected.total[, news_source_is_twitter := main_information_sorce == "Social Media"]
d.affected.total[, voted_preference.2012 := vote2012 == voting_preference ]
d.affected.total[, voted_preference.2016 := vote2016 == voting_preference ]
model.did <- lm(data = d.affected.total, voted_preference.2016 ~ affected + 
                  voted_preference.2012 + voted_preference.2012:affected)
stargazer(model.did, type="text")
```

## Credibility
```{r}
p.1 <- get.percentage(d.affected.total[voted_preference.2012 == TRUE & affected == TRUE,], 
                      d.affected.total[voted_preference.2012 == TRUE,])
p.2 <- get.percentage(d.affected.total[voted_preference.2012 == TRUE & affected == FALSE,], 
                      d.affected.total[voted_preference.2012 == TRUE,])
p.3 <- get.percentage(d.affected.total[voted_preference.2012 == FALSE & affected == TRUE,], 
                      d.affected.total[voted_preference.2012 == FALSE,])
p.4 <- get.percentage(d.affected.total[voted_preference.2012 == FALSE & affected == FALSE,], 
                      d.affected.total[voted_preference.2012 == FALSE,])
cat(paste(p.1,p.2,p.3,p.4, sep = "\n"))
```

> As we can see, both relative populations between those who were affected and those who weren't affected for both our 
> control and treatment populations are nearly identical with both affected participants having nearly identical percentages 
> at ``r p.1`` and ``r p.3``. Similarly both groups of non-affected participants have percentages at ``r p.2`` and ``r p.4``.

## Conclusions
The size of the effect between voting behavior and those affected by troll tweet exposure is significant and shows
evidence for our original hypothesis. Challenges in collecting actual data: the preference survey is a sensitive issue
because it is political in nature which can cause a problems for data collection. 

# Policy to Mitigate a Cyber Threat
## Existing Studies
Describe the major results from the existing studies (academic papers or reports) regarding the mitigation of your cyber threat

> Partnerships with social media platforms for Curbing social bots may be an effective strategy for mitigating the spread of online misinformation.

Describe the data used in these studies, Justify that the relationship between a policy and the scale of a cyber threat are causal, potential concerns with data

> There are no existing studies that can help us specify data for the mitigation of cyber threat.


## Ideal Experiment

+ We would partner with twitter to run an experiment on a subset of our current population. In this experiment, people
  would be exposed to fake tweets and legit tweets (all of them generated by us), and we would collect the following
  columns:
  + Number of fake troll tweets seen
  + Number of fake troll tweets reported
  + Number of fake troll tweets interacted with
  + Plan to vote for in 2024 (after the experiment period ends)
+ The control group would be those people that see fake troll tweets but do not know whether they are troll tweets or
  not. They can interact with them but have no reporting capabilities nor learning material about the troll tweets.
+ The treatment group would be those people that see fake troll tweets but can check whether they are legit tweets or
  troll tweet. They can learn overtime from experience as well as from resources provided from within twitter’s platform
  such as information campaigns.
+ We would use a difference in differences approach - how many people changed their voting preference in the control
  group vs. how many people changed their preference in the treatment group.
+ The biggest concern is getting Twitter support for this. Everyone that is enrolled knows that they are part of an
  experiment. We would need to employ people to somehow generate the fake tweets or create a program that does it for
  us. It is probably unfeasible for Twitter to allow a 3rd party to conduct research about how their platform operates.

## Existing Data Sources
+ There are no existing sources to collect a dataset that could help us establish the impact of our policy since it is a
  new experiment that would need to be run first in order to collect and analyze the data and the impact of the policy
  on the cyber threat.
+ The structure of our dataset will consist of these  eight  columns:
  + general_voting_preference
  + number_of_troll_tweets_seen
  + number_of_troll_tweets_interacted_with
  + number_of_troll_tweets_reported
  + voting_plan_2024
  + treated
  + voting_plan_different
  + affected
+ Anyone who has the treated column as true, is part of the treatment group. This is regardless as to whether or not
  they reported any tweets or interacted with any tweets.
+ The outcome of interest would be a DID model comparing whether or not someone was affected by the troll tweets seen
  (number_of_troll_tweets_seen > mean(troll tweets seen)) combined with whether or not the user is part of the treatment
  group or not. 
+ These two variables would describe as to whether the voting plan was different or not.

## Research Design
+ We used python to randomly create the control and treatment group as well as the results of the experiment. We used
  the DiD model to show that there is a difference between those who are part of the treatment and those who are not
  part of it. 
+ We randomly selected people from the population, and that we randomly showed them legitimate and fake troll tweets.
  The control group was always given the same chance to interact with them without any knowledge of the tweets being
  fake or real. 

## Regression Table
Skipped as per instructions

## Credibility
Skipped as per instructions

## Conclusions
+ Although this data was generated with the intension to show that this experiment would succeed, we believe the
  experiment could work and a real research experiment and real data are needed in order to come up to a solid
  conclusion.
+ The main challenge we face in collecting this data is Twitter support to allow us to run the experiment.

\newpage
# Bibliography